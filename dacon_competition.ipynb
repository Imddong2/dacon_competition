{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import librosa\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "africa_train_paths = glob(\"./open/train/africa/*.wav\")\n",
    "australia_train_paths = glob(\"./open/train/australia/*.wav\")\n",
    "canada_train_paths = glob(\"./open/train/canada/*.wav\")\n",
    "england_train_paths = glob(\"./open/train/england/*.wav\")\n",
    "hongkong_train_paths = glob(\"./open/train/hongkong/*.wav\")\n",
    "us_train_paths = glob(\"./open/train/us/*.wav\")\n",
    "\n",
    "path_list = [africa_train_paths, australia_train_paths, canada_train_paths,\n",
    "             england_train_paths, hongkong_train_paths, us_train_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./open/test/4992.wav</td>\n",
       "      <td>4992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./open/test/4354.wav</td>\n",
       "      <td>4354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./open/test/5830.wav</td>\n",
       "      <td>5830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./open/test/3350.wav</td>\n",
       "      <td>3350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./open/test/3675.wav</td>\n",
       "      <td>3675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   path    id\n",
       "0  ./open/test/4992.wav  4992\n",
       "1  ./open/test/4354.wav  4354\n",
       "2  ./open/test/5830.wav  5830\n",
       "3  ./open/test/3350.wav  3350\n",
       "4  ./open/test/3675.wav  3675"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glob로 test data의 path를 불러올때 순서대로 로드되지 않을 경우를 주의해야 합니다.\n",
    "# test_ 데이터 프레임을 만들어서 나중에 sample_submission과 id를 기준으로 merge시킬 준비를 합니다.\n",
    "def get_id(data):\n",
    "    return np.int(data.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "test_ = pd.DataFrame(index = range(0, 6100), columns = [\"path\", \"id\"])\n",
    "test_[\"path\"] = glob(\"./open/test/*.wav\")\n",
    "test_[\"id\"] = test_[\"path\"].apply(lambda x : get_id(x))\n",
    "\n",
    "test_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "baseline 코드에서는 librosa 라이브러리를 사용하여 wav파일을 전처리 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "\n",
    "    result = []\n",
    "    for path in tqdm(paths):\n",
    "        # sr = 16000이 의미하는 것은 1초당 16000개의 데이터를 샘플링 한다는 것입니다.\n",
    "        data, sr = librosa.load(path, sr = 16000)\n",
    "        result.append(data)\n",
    "    result = np.array(result) \n",
    "    # 메모리가 부족할 때는 데이터 타입을 변경해 주세요 ex) np.array(data, dtype = np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [06:58<00:00,  5.97it/s]\n",
      "100%|██████████| 1000/1000 [02:44<00:00,  6.08it/s]\n",
      "100%|██████████| 1000/1000 [02:43<00:00,  6.11it/s]\n",
      "100%|██████████| 10000/10000 [27:26<00:00,  6.07it/s] \n",
      "100%|██████████| 1020/1020 [02:54<00:00,  5.83it/s]\n",
      "100%|██████████| 10000/10000 [27:13<00:00,  6.12it/s] \n",
      "100%|██████████| 6100/6100 [16:53<00:00,  6.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# train 데이터를 로드하기 위해서는 많은 시간이 소모 됩니다.\n",
    "# 따라서 추출된 정보를 npy파일로 저장하여 필요 할 때마다 불러올 수 있게 준비합니다.\n",
    "\n",
    "os.mkdir(\"./npy_data\")\n",
    "\n",
    "africa_train_data = load_data(africa_train_paths)\n",
    "\n",
    "np.save(\"./npy_data/africa_npy\", africa_train_data)\n",
    "\n",
    "australia_train_data = load_data(australia_train_paths)\n",
    "np.save(\"./npy_data/australia_npy\", australia_train_data)\n",
    "\n",
    "canada_train_data = load_data(canada_train_paths)\n",
    "np.save(\"./npy_data/canada_npy\", canada_train_data)\n",
    "\n",
    "england_train_data = load_data(england_train_paths)\n",
    "np.save(\"./npy_data/england_npy\", england_train_data)\n",
    "\n",
    "hongkong_train_data = load_data(hongkong_train_paths)\n",
    "np.save(\"./npy_data/hongkong_npy\", hongkong_train_data)\n",
    "\n",
    "us_train_data = load_data(us_train_paths)\n",
    "np.save(\"./npy_data/us_npy\", us_train_data)\n",
    "\n",
    "test_data = load_data(test_[\"path\"])\n",
    "np.save(\"./npy_data/test_npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy파일로 저장된 데이터를 불러옵니다.\n",
    "africa_train_data = np.load(\"./npy_data/africa_npy.npy\", allow_pickle = True)\n",
    "australia_train_data = np.load(\"./npy_data/australia_npy.npy\", allow_pickle = True)\n",
    "canada_train_data = np.load(\"./npy_data/canada_npy.npy\", allow_pickle = True)\n",
    "england_train_data = np.load(\"./npy_data/england_npy.npy\", allow_pickle = True)\n",
    "hongkong_train_data = np.load(\"./npy_data/hongkong_npy.npy\", allow_pickle = True)\n",
    "us_train_data = np.load(\"./npy_data/us_npy.npy\", allow_pickle = True)\n",
    "test_data = np.load(\"./npy_data/test_npy.npy\", allow_pickle = True)\n",
    "\n",
    "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 대회에서 음성은 각각 다른 길이를 갖고 있습니다.\n",
    "# baseline 코드에서는 음성 중 길이가 가장 작은 길이의 데이터를 기준으로 데이터를 잘라서 사용합니다.\n",
    "def get_mini(data):\n",
    "    mini = 9999999\n",
    "    for i in data:\n",
    "        if len(i) < mini:\n",
    "            mini = len(i)\n",
    "\n",
    "    return mini\n",
    "\n",
    "#음성들의 길이를 맞춰줍니다.\n",
    "def set_length(data, d_mini):\n",
    "    result = []\n",
    "    for i in data:\n",
    "        result.append(i[:d_mini])\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "#feature를 생성합니다.\n",
    "def get_feature(data, sr = 16000, n_fft = 256, win_length = 200, hop_length = 160, n_mels = 64):\n",
    "    mel = []\n",
    "    for i in data:\n",
    "        # win_length 는 음성을 작은 조각으로 자를때 작은 조각의 크기입니다.\n",
    "        # hop_length 는 음성을 작은 조각으로 자를때 자르는 간격을 의미합니다.\n",
    "        # n_mels 는 적용할 mel filter의 개수입니다.\n",
    "        mel_ = librosa.feature.melspectrogram(i, sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
    "        mel.append(mel_)\n",
    "    mel = np.array(mel)\n",
    "    mel = librosa.power_to_db(mel, ref = np.max)\n",
    "\n",
    "    mel_mean = mel.mean()\n",
    "    mel_std = mel.std()\n",
    "    mel = (mel - mel_mean) / mel_std\n",
    "\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#음성인식에서 효과를 보였던 specaugment를 적용시켜보았습니다.\n",
    "#시간축과 주파수축을 랜덤으로 지정하여 0으로 마스킹시킵니다.\n",
    "#SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition(https://arxiv.org/pdf/1904.08779.pdf)\n",
    "def spec_augment(wav, T=40, F=15, time_mask_num=1, freq_mask_num=1):\n",
    "    feat_size = wav.shape[0]\n",
    "    seq_len = wav.shape[1]\n",
    "    # time mask\n",
    "    for _ in range(time_mask_num):\n",
    "        t = np.random.uniform(low=0.0, high=T)\n",
    "        t = int(t)\n",
    "        t0 = random.randint(0, seq_len - t)\n",
    "        wav[t0 : t0 + t] = 0\n",
    "    # freq mask\n",
    "    for _ in range(freq_mask_num):\n",
    "        f = np.random.uniform(low=0.0, high=F)\n",
    "        f = int(f)\n",
    "        f0 = random.randint(0, feat_size - f)\n",
    "        wav[:, f0 : f0 + f] = 0\n",
    "    return wav\n",
    "\n",
    "def augmentation(data):\n",
    "    aug = []\n",
    "    for i in data:\n",
    "        aug_ = spec_augment(i)\n",
    "        aug.append(aug_)\n",
    "    aug = np.array(aug)\n",
    "    return aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x = np.concatenate(train_data_list, axis= 0)\n",
    "test_x = np.array(test_data)\n",
    "\n",
    "\n",
    "# 음성의 길이 중 가장 작은 길이를 구합니다.\n",
    "train_mini = get_mini(train_x)\n",
    "test_mini = get_mini(test_x)\n",
    "\n",
    "mini = np.min([train_mini, test_mini])\n",
    "\n",
    "# data의 길이를 가장 작은 길이에 맞춰 잘라줍니다.\n",
    "train_x = set_length(train_x, mini)\n",
    "test_x = set_length(test_x, mini)\n",
    "\n",
    "# librosa를 이용해 feature를 추출합니다.\n",
    "train_x = get_feature(data = train_x)\n",
    "test_x = get_feature(data = test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(-1, train_x.shape[1], train_x.shape[2], 1)\n",
    "test_x = test_x.reshape(-1, test_x.shape[1], test_x.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data의 label을 생성해 줍니다.\n",
    "train_y = np.concatenate((np.zeros(len(africa_train_data), dtype = np.int),\n",
    "                        np.ones(len(australia_train_data), dtype = np.int),\n",
    "                         np.ones(len(canada_train_data), dtype = np.int) * 2,\n",
    "                         np.ones(len(england_train_data), dtype = np.int) * 3,\n",
    "                         np.ones(len(hongkong_train_data), dtype = np.int) * 4,\n",
    "                         np.ones(len(us_train_data), dtype = np.int) * 5), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25520, 64, 501, 1), (25520,), (6100, 64, 501, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분석 모델(baseline모델 사용, 변경x)\n",
    "분석 모델은 월간데이콘_6 음성 중첩 데이터 분류 AI 경진대회 3위를 달성하신 Jamm님의 코드를 바탕으로 만들어졌습니다.\n",
    "https://www.dacon.io/competitions/official/235616/codeshare/1571?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Convolution2D, BatchNormalization, Flatten,\n",
    "                                     Dropout, Dense, AveragePooling2D, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(input_, units = 32, dropout_rate = 0.5):\n",
    "    \n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(input_)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, x_res])\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def second_block(input_, units = 64, dropout_rate = 0.5):\n",
    "    \n",
    "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(input_)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(units, 1, padding = \"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units * 4, 1, padding = \"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, x_res])\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fn():\n",
    "    dropout_rate = 0.3\n",
    "    \n",
    "    in_ = Input(shape = (train_x.shape[1:]))\n",
    "    \n",
    "    block_01 = block(in_, units = 32, dropout_rate = dropout_rate)\n",
    "    block_02 = block(block_01, units = 64, dropout_rate = dropout_rate)\n",
    "    block_03 = block(block_02, units = 128, dropout_rate = dropout_rate)\n",
    "\n",
    "    block_04 = second_block(block_03, units = 64, dropout_rate = dropout_rate)\n",
    "    block_05 = second_block(block_04, units = 128, dropout_rate = dropout_rate)\n",
    "\n",
    "    x = Flatten()(block_05)\n",
    "\n",
    "    x = Dense(units = 128, activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Dropout(rate = dropout_rate)(x)\n",
    "\n",
    "    x = Dense(units = 128, activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x_res, x])\n",
    "    x = Dropout(rate = dropout_rate)(x)\n",
    "\n",
    "    model_out = Dense(units = 6, activation = 'softmax')(x)\n",
    "    model = Model(in_, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1276/1276 [==============================] - 68s 50ms/step - loss: 1.4561 - acc: 0.4153 - val_loss: 1.4568 - val_acc: 0.4189\n",
      "Epoch 2/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 1.2305 - acc: 0.5088 - val_loss: 1.2670 - val_acc: 0.4790\n",
      "Epoch 3/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 1.1038 - acc: 0.5780 - val_loss: 1.0660 - val_acc: 0.6021\n",
      "Epoch 4/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 1.0038 - acc: 0.6236 - val_loss: 1.4979 - val_acc: 0.4614\n",
      "Epoch 5/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 0.9808 - acc: 0.6320 - val_loss: 2.3398 - val_acc: 0.3080\n",
      "Epoch 6/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 0.8614 - acc: 0.6805 - val_loss: 0.9732 - val_acc: 0.6511\n",
      "Epoch 7/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 0.7690 - acc: 0.7138 - val_loss: 1.0294 - val_acc: 0.6328\n",
      "Epoch 8/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 0.7107 - acc: 0.7366 - val_loss: 0.9060 - val_acc: 0.6757\n",
      "*******************************************************************\n",
      "*******************************************************************\n",
      "Epoch 1/8\n",
      "1276/1276 [==============================] - 66s 51ms/step - loss: 1.4458 - acc: 0.4149 - val_loss: 1.3121 - val_acc: 0.4461\n",
      "Epoch 2/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 1.2406 - acc: 0.4921 - val_loss: 1.2689 - val_acc: 0.4957\n",
      "Epoch 3/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 1.1799 - acc: 0.5300 - val_loss: 2.2968 - val_acc: 0.3452\n",
      "Epoch 4/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 1.0914 - acc: 0.5831 - val_loss: 1.1849 - val_acc: 0.5586\n",
      "Epoch 5/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 0.9969 - acc: 0.6267 - val_loss: 1.1094 - val_acc: 0.5878\n",
      "Epoch 6/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 0.9307 - acc: 0.6482 - val_loss: 1.0398 - val_acc: 0.6101\n",
      "Epoch 7/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 0.8463 - acc: 0.6809 - val_loss: 1.0023 - val_acc: 0.6399\n",
      "Epoch 8/8\n",
      "1276/1276 [==============================] - 64s 50ms/step - loss: 0.7576 - acc: 0.7168 - val_loss: 1.7391 - val_acc: 0.5613\n",
      "*******************************************************************\n",
      "*******************************************************************\n",
      "Epoch 1/8\n",
      "1276/1276 [==============================] - 65s 50ms/step - loss: 1.4441 - acc: 0.4062 - val_loss: 1.3289 - val_acc: 0.4549\n",
      "Epoch 2/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 1.2545 - acc: 0.4857 - val_loss: 2.2589 - val_acc: 0.3103\n",
      "Epoch 3/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 1.1484 - acc: 0.5446 - val_loss: 1.1867 - val_acc: 0.5566\n",
      "Epoch 4/8\n",
      "1276/1276 [==============================] - 63s 49ms/step - loss: 1.0571 - acc: 0.5938 - val_loss: 2.1587 - val_acc: 0.4589\n",
      "Epoch 5/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 0.9860 - acc: 0.6283 - val_loss: 1.6575 - val_acc: 0.4818\n",
      "Epoch 6/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 0.8922 - acc: 0.6675 - val_loss: 0.9905 - val_acc: 0.6295\n",
      "Epoch 7/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 0.8041 - acc: 0.7017 - val_loss: 0.8805 - val_acc: 0.6791\n",
      "Epoch 8/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 0.7239 - acc: 0.7270 - val_loss: 0.8592 - val_acc: 0.6846\n",
      "*******************************************************************\n",
      "*******************************************************************\n",
      "Epoch 1/8\n",
      "1276/1276 [==============================] - 65s 50ms/step - loss: 1.4590 - acc: 0.4214 - val_loss: 1.3960 - val_acc: 0.3924\n",
      "Epoch 2/8\n",
      "1276/1276 [==============================] - 63s 49ms/step - loss: 1.2344 - acc: 0.5083 - val_loss: 1.2016 - val_acc: 0.5210\n",
      "Epoch 3/8\n",
      "1276/1276 [==============================] - 63s 49ms/step - loss: 1.1224 - acc: 0.5746 - val_loss: 1.3726 - val_acc: 0.4522\n",
      "Epoch 4/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 1.0693 - acc: 0.5965 - val_loss: 1.1708 - val_acc: 0.5694\n",
      "Epoch 5/8\n",
      "1276/1276 [==============================] - 63s 49ms/step - loss: 0.9762 - acc: 0.6359 - val_loss: 0.9780 - val_acc: 0.6552\n",
      "Epoch 6/8\n",
      "1276/1276 [==============================] - 63s 49ms/step - loss: 0.8923 - acc: 0.6657 - val_loss: 1.0525 - val_acc: 0.6540\n",
      "Epoch 7/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 0.8434 - acc: 0.6865 - val_loss: 1.0528 - val_acc: 0.6583\n",
      "Epoch 8/8\n",
      "1276/1276 [==============================] - 63s 49ms/step - loss: 0.7467 - acc: 0.7253 - val_loss: 1.0886 - val_acc: 0.6454\n",
      "*******************************************************************\n",
      "*******************************************************************\n",
      "Epoch 1/8\n",
      "1276/1276 [==============================] - 65s 50ms/step - loss: 1.4770 - acc: 0.4348 - val_loss: 1.4525 - val_acc: 0.4218\n",
      "Epoch 2/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 1.2297 - acc: 0.5169 - val_loss: 1.2481 - val_acc: 0.5217\n",
      "Epoch 3/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 1.1219 - acc: 0.5765 - val_loss: 1.6092 - val_acc: 0.4704\n",
      "Epoch 4/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 1.0386 - acc: 0.6150 - val_loss: 1.1059 - val_acc: 0.6174\n",
      "Epoch 5/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 0.9628 - acc: 0.6440 - val_loss: 0.9549 - val_acc: 0.6428\n",
      "Epoch 6/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 0.8788 - acc: 0.6784 - val_loss: 1.2087 - val_acc: 0.5825\n",
      "Epoch 7/8\n",
      "1276/1276 [==============================] - 63s 50ms/step - loss: 0.8065 - acc: 0.7039 - val_loss: 1.0749 - val_acc: 0.6234\n",
      "Epoch 8/8\n",
      "1276/1276 [==============================] - 63s 49ms/step - loss: 0.7237 - acc: 0.7327 - val_loss: 1.7925 - val_acc: 0.5331\n",
      "*******************************************************************\n",
      "*******************************************************************\n"
     ]
    }
   ],
   "source": [
    "split = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 10)\n",
    "\n",
    "pred = []\n",
    "pred_ = []\n",
    "\n",
    "for train_idx, val_idx in split.split(train_x, train_y):\n",
    "    x_train, y_train = train_x[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train_x[val_idx], train_y[val_idx]\n",
    "    \n",
    "    \n",
    "    #제공 받은 음성데이터는 국적별로 데이터 불균형이 심하기 떄문에 augmentation을 시켜 데이터 수를 늘렸습니다.\n",
    "    x_train_aug = augmentation(data = x_train)\n",
    "    x_train = np.append(x_train, x_train_aug, axis=0)\n",
    "    y_train = np.append(y_train, y_train, axis=0)\n",
    "\n",
    "\n",
    "    model = build_fn()\n",
    "    model.compile(optimizer = keras.optimizers.Adam(0.002),\n",
    "                 loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics = ['acc'])\n",
    "\n",
    "    history = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val), epochs = 8)\n",
    "    print(\"*******************************************************************\")\n",
    "    pred.append(model.predict(test_x))\n",
    "    pred_.append(np.argmax(model.predict(test_x), axis = 1))\n",
    "    print(\"*******************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_type(data):\n",
    "    return np.int(data)\n",
    "\n",
    "# 처음에 살펴본 것처럼 glob로 test data의 path는 sample_submission의 id와 같이 1,2,3,4,5.....으로 정렬 되어있지 않습니다.\n",
    "# 만들어둔 test_ 데이터프레임을 이용하여 sample_submission과 predict값의 id를 맞춰줍니다.\n",
    "\n",
    "result = pd.concat([test_, pd.DataFrame(np.mean(pred, axis = 0))], axis = 1).iloc[:, 1:]\n",
    "result[\"id\"] = result[\"id\"].apply(lambda x : cov_type(x))\n",
    "\n",
    "result = pd.merge(sample_submission[\"id\"], result)\n",
    "result.columns = sample_submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>africa</th>\n",
       "      <th>australia</th>\n",
       "      <th>canada</th>\n",
       "      <th>england</th>\n",
       "      <th>hongkong</th>\n",
       "      <th>us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.258713</td>\n",
       "      <td>0.058092</td>\n",
       "      <td>0.006629</td>\n",
       "      <td>0.198833</td>\n",
       "      <td>0.055892</td>\n",
       "      <td>0.421840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.022082</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>0.565977</td>\n",
       "      <td>0.006656</td>\n",
       "      <td>0.332757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.110126</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.817054</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.062029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.040056</td>\n",
       "      <td>0.018782</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.735988</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.199907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.031758</td>\n",
       "      <td>0.011162</td>\n",
       "      <td>0.018954</td>\n",
       "      <td>0.125401</td>\n",
       "      <td>0.048576</td>\n",
       "      <td>0.764150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6095</th>\n",
       "      <td>6096</td>\n",
       "      <td>0.032152</td>\n",
       "      <td>0.034568</td>\n",
       "      <td>0.005660</td>\n",
       "      <td>0.295389</td>\n",
       "      <td>0.243007</td>\n",
       "      <td>0.389224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>6097</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.176527</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.816032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>6098</td>\n",
       "      <td>0.017174</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.894320</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.082689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>6099</td>\n",
       "      <td>0.043688</td>\n",
       "      <td>0.008637</td>\n",
       "      <td>0.013291</td>\n",
       "      <td>0.454267</td>\n",
       "      <td>0.018703</td>\n",
       "      <td>0.461414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6099</th>\n",
       "      <td>6100</td>\n",
       "      <td>0.018103</td>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>0.130683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    africa  australia    canada   england  hongkong        us\n",
       "0        1  0.258713   0.058092  0.006629  0.198833  0.055892  0.421840\n",
       "1        2  0.022082   0.061674  0.010854  0.565977  0.006656  0.332757\n",
       "2        3  0.110126   0.000933  0.000228  0.817054  0.009630  0.062029\n",
       "3        4  0.040056   0.018782  0.000304  0.735988  0.004963  0.199907\n",
       "4        5  0.031758   0.011162  0.018954  0.125401  0.048576  0.764150\n",
       "...    ...       ...        ...       ...       ...       ...       ...\n",
       "6095  6096  0.032152   0.034568  0.005660  0.295389  0.243007  0.389224\n",
       "6096  6097  0.000513   0.002838  0.002620  0.176527  0.001470  0.816032\n",
       "6097  6098  0.017174   0.000409  0.000494  0.894320  0.004913  0.082689\n",
       "6098  6099  0.043688   0.008637  0.013291  0.454267  0.018703  0.461414\n",
       "6099  6100  0.018103   0.003345  0.000304  0.818792  0.028773  0.130683\n",
       "\n",
       "[6100 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"DACON_v4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
